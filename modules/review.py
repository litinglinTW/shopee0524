"""
Created on Sun May 11 10:38:40 2025
 
@æ”¹author: xæ—ä¿å©·

å•†å“ç•™è¨€åˆ†æå™¨(api)ï¼š# review.py

æª”æ¡ˆé–‹å•Ÿä¿®æ”¹ç´€éŒ„ï¼š
0522 5:03 pm. ver(by æ´ªåŠ›å®‰)ï¼ˆç›´æˆªçˆ¬å‰å°ï¼‰

0523 
11:03 am.  (by æ´ªåŠ›å®‰)ï¼‹ä¾‹å¤–è™•ç†ï¼Œå•Ÿå‹•å¤±æ•—æ™‚æ¸…æ¥šé¡¯ç¤ºå•é¡Œ
12:03 am.  æ”¹ç‰ˆï¼šapi

0524 
11:12 am. æ”¹ç‰ˆï¼šapiã€‚classç‰ˆæœ¬ï¼ˆchatGPTç”Ÿæˆï¼‰
02:12 pm.  Cookie 
"""
'''


from config.config import settings
from view.utils import timer
from view.check_ip_pool import CheckIPAddress
from view.api_v4_get_shop_detail import ShopDetailCrawler
from view.api_v4_get_product_detail import ProductDetailCrawler


import logging


'''
import streamlit as st
import asyncio
import aiohttp
import logging
import pandas as pd
from datetime import datetime
from pydantic import BaseModel, ValidationError
import matplotlib.pyplot as plt
import seaborn as sns
import re
import json

# è¨­å®š logging æ ¼å¼èˆ‡ç­‰ç´š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ================= ä½¿ç”¨èªªæ˜ =================
def show_cookie_instructions():
    st.markdown("""
    (æœ€å¾Œç·¨è¼¯ï¼š20250524 08:41pm)
    
**å¦‚ä½•å–å¾— Shopee Cookieï¼š**

1. é–‹å•Ÿç€è¦½å™¨ï¼ˆå»ºè­° Chromeï¼‰ï¼Œé€²å…¥ [è¦çš®é¦–é ](https://shopee.tw)ã€‚
2. é»å³ä¸Šè§’ã€Œç™»å…¥ã€ä¸¦å®Œæˆç™»å…¥ï¼ˆå¯ç”¨ Googleã€FBã€æ‰‹æ©Ÿè™Ÿç­‰æ–¹å¼ï¼‰ã€‚
3. ç™»å…¥å¾Œï¼ŒæŒ‰ä¸‹éµç›¤ `F12` æˆ–é»å³éµé¸ã€Œæª¢æŸ¥ã€æ‰“é–‹é–‹ç™¼è€…å·¥å…·ã€‚
4. é»é¸ã€ŒApplicationã€ï¼ˆä¸­æ–‡å¯èƒ½æ˜¯ã€Œæ‡‰ç”¨ç¨‹å¼ã€ï¼‰åˆ†é ï¼Œå†é»å·¦å´ã€ŒCookiesã€å±•é–‹ï¼Œé¸æ“‡ `https://shopee.tw`ã€‚
5. åœ¨å³å´çœ‹åˆ°å¤šçµ„ cookieï¼ˆå¦‚ SPC_ECã€SPC_Uã€SPC_F ç­‰ï¼‰ï¼Œé¸ä¸€çµ„å…¨éƒ¨è¤‡è£½ï¼ˆå¯å…¨éƒ¨ Ctrl+A, Ctrl+Cï¼‰ï¼Œè²¼åˆ°ä¸‹æ–¹ã€ŒCookieã€æ¬„ä½ã€‚
        cookieè¤‡è£½éä¾†çš„æ ¼å¼ï¼šSPC_F=xxxxxx; SPC_U=yyyyyy; SPC_EC=zzzzzz; SPC_R_T_ID=aaaaaa; SPC_R_T_IV=bbbbbb; SPC_T_ID=cccccc; SPC_T_IV=dddddd; ...
        ï¼ˆå¯ä»¥è«‹AIå¹«å¿™æª¢æŸ¥è¤‡è£½å…§å®¹æ˜¯å¦ç¬¦åˆæ ¼å¼ï¼‰
6. è‹¥ç”¨ Chrome æ“´å……å¥—ä»¶å¦‚ EditThisCookie ä¹Ÿå¯ä¸€éµè¤‡è£½æ‰€æœ‰ cookieã€‚

> **è­¦å‘Šï¼š** æ²’æœ‰è²¼ å…¨æ–°ã€é¦¬ä¸Šè¤‡è£½ä¸‹ä¾†çš„Cookieï¼ŒåŸºæœ¬ä¸Šè·‘ä¸å‹•ã€‚æ²’è²¼çš„è©±é è¨­æœƒç”¨å…§å»º Cookieï¼ˆæˆåŠŸç‡0.01ï¼Œå› ç‚ºæˆ‘çš„å¸³è™Ÿå·²ç¶“è¢«è¦çš®åµæ¸¬åˆ°ã„Œï¼‰ã€‚

---
""", unsafe_allow_html=True)


# ================= çˆ¬ï¼†è¼¸å‡ºæª”æ¡ˆ =================

# ç•™è¨€è³‡æ–™æ¨¡å‹
class CommentData(BaseModel):
    cmtid: int
    author_username: str
    comment: str
    rating_star: int
    ctime: int
    comment_time_str: str  # è½‰æ›å¾Œçš„æ™‚é–“å­—ä¸²

    @classmethod
    def from_api(cls, data: dict):
        ctime = data.get("ctime", 0)
        comment_time_str = datetime.utcfromtimestamp(ctime).strftime("%Y-%m-%d %H:%M:%S")
        return cls(
            cmtid=data.get("cmtid"),
            author_username=data.get("author_username", "åŒ¿å"),
            comment=data.get("comment", ""),
            rating_star=data.get("rating_star", 0),
            ctime=ctime,
            comment_time_str=comment_time_str,
        )

class ShopeeCommentCrawler:
    def __init__(self, cookie: str = "", product_url: str = "", proxy: str = None, base_api=None, retries=3, retry_delay=2):
        self.base_api = base_api or "https://shopee.tw/api/v2/item/get_ratings"
        self.headers = {
            "accept": "application/json",
            "accept-encoding": "gzip, deflate, br, zstd",
            "accept-language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
            "af-ac-enc-dat": "ed556fa01579fe18",
            "af-ac-enc-sz-token": "LFu9USeoQqRsXbyJoI5vkA==|6OZoc7UzEUXdy3tJvq1NoCnC6V/xwsVebuWnTPWwGTUff08xGmbMUKjqjp7r8/UCcbVyaktThVZcVPQYugo=|D/Ep+pK9EqptTiDz|08|3",
            "content-type": "application/json",
            "priority": "u=1, i",
            "sec-ch-ua": '"Chromium";v="136", "Google Chrome";v="136", "Not.A/Brand";v="99"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"macOS"',
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-origin",
            "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36",
            "x-api-source": "pc",
            "x-requested-with": "XMLHttpRequest",
            "x-sap-ri": "f86e31684cf9afa65bc43f3a0601fecd62195ec66b4defec64d3",
            "x-sap-sec": "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",
            "x-shopee-language": "zh-Hant",
            "x-sz-sdk-version": "1.12.19"
        }
        # ä¾ç”¨æˆ¶å‚³å…¥çš„ç¶²å€å¡« referer
        if product_url:
            self.headers["referer"] = product_url

        # ä¾ç”¨æˆ¶ cookie å¡« cookie
        if cookie:
            # å…ˆå»é™¤ cookie æ›è¡Œ
            cookie = cookie.replace('\n', '').replace('\r', '')
            self.headers["cookie"] = cookie
            # è‡ªå‹•å–å¾— csrftoken è£œé€² x-csrftoken
            match = re.search(r'csrftoken=([^;]+)', cookie)
            if match:
                self.headers["x-csrftoken"] = match.group(1)
            elif "x-csrftoken" in self.headers:
                del self.headers["x-csrftoken"]
        else:
            # é è¨­ cookie
            self.headers["cookie"] = "SPC_F=fXCf8Ud1RioYc8yRE1TmF7B5AWSaVHru; SPC_U=1549828388;"
            if "x-csrftoken" in self.headers:
                del self.headers["x-csrftoken"]

        self.proxy = proxy
        self.retries = retries
        self.retry_delay = retry_delay
        self.comments = []

        # çµ±ä¸€é˜²å‘†ï¼Œheaders è£¡é¢æ‰€æœ‰ value éƒ½å»é™¤ \n \r
        for k, v in self.headers.items():
            if isinstance(v, str):
                self.headers[k] = v.replace('\n', '').replace('\r', '')

    

    async def fetch_comments(self, session, shopid, itemid, offset=0, limit=50):
        params = {
            "filter": 0,
            "flag": 1,
            "itemid": itemid,
            "limit": limit,
            "offset": offset,
            "shopid": shopid,
            "type": 0,
        }
        for attempt in range(1, self.retries + 1):
            resp = None
            api_raw_response = None
            try:
                async with session.get(self.base_api, params=params, proxy=self.proxy, timeout=10, headers=self.headers) as resp:
                    status = resp.status
                    api_raw_response = await resp.text()
                    if status == 403:
                        logger.error("HTTP 403 Forbiddenï¼šå¯èƒ½æ˜¯IPè¢«è¦çš®å°é–ã€Cookieå¤±æ•ˆã€æˆ–Headerä¸å®Œæ•´")
                        return {"error": "HTTP 403 Forbiddenï¼šå¯èƒ½æ˜¯IPè¢«è¦çš®å°é–ã€Cookieå¤±æ•ˆã€æˆ–Headerä¸å®Œæ•´", "api_raw_response": api_raw_response}
                    elif status == 429:
                        logger.error("HTTP 429 Too Many Requestsï¼šè¢«è¦çš®æš«æ™‚é™åˆ¶ï¼Œè«‹é™ä½æŠ“å–é »ç‡æˆ–æ›´æ›IP")
                        return {"error": "HTTP 429 Too Many Requestsï¼šè¢«è¦çš®æš«æ™‚é™åˆ¶ï¼Œè«‹é™ä½æŠ“å–é »ç‡æˆ–æ›´æ›IP", "api_raw_response": api_raw_response}
                    elif status == 401:
                        logger.error("HTTP 401 Unauthorizedï¼šCookie ç„¡æ•ˆæˆ–æœªç™»å…¥")
                        return {"error": "HTTP 401 Unauthorizedï¼šCookie ç„¡æ•ˆæˆ–æœªç™»å…¥", "api_raw_response": api_raw_response}
                    elif status != 200:
                        logger.warning(f"HTTP error {status} for offset {offset}, attempt {attempt}/{self.retries}")
                        return {"error": f"HTTP error code {status}ï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ–Cookie/Headerè¨­ç½®", "api_raw_response": api_raw_response}
                    else:
                        try:
                            data = json.loads(api_raw_response)
                        except Exception as e:
                            logger.error(f"APIå›æ‡‰ä¸æ˜¯æœ‰æ•ˆJSONï¼š{e}")
                            return {"error": f"APIå›æ‡‰ä¸æ˜¯æœ‰æ•ˆJSONï¼š{e}", "api_raw_response": api_raw_response}
                        if "data" not in data or data["data"] is None:
                            logger.error(f"APIåŸå§‹å›æ‡‰å…§å®¹ï¼š{api_raw_response}")
                            logger.error("APIå›å‚³æ ¼å¼ç•°å¸¸ï¼Œå¯èƒ½æ˜¯Headerä¸è¶³æˆ–Cookieå¤±æ•ˆ")
                            return {"error": "APIå›å‚³æ ¼å¼ç•°å¸¸ï¼Œå¯èƒ½æ˜¯Headerä¸è¶³æˆ–Cookieå¤±æ•ˆ", "api_raw_response": api_raw_response}
                        if "ratings" in data["data"] and not data["data"]["ratings"]:
                            logger.info("APIæ­£å¸¸å›å‚³ä½†ç„¡ç•™è¨€ï¼Œå¯èƒ½æ˜¯å•†å“ç„¡ç•™è¨€æˆ–ç•™è¨€æœªå…¬é–‹")
                            return {"error": "APIæ­£å¸¸å›å‚³ä½†ç„¡ç•™è¨€ï¼Œå¯èƒ½æ˜¯å•†å“ç„¡ç•™è¨€æˆ–ç•™è¨€æœªå…¬é–‹", "api_raw_response": api_raw_response}
                        return {"data": data, "api_raw_response": api_raw_response}
            except Exception as e:
                if resp is not None:
                    try:
                        api_raw_response = await resp.text()
                        logger.error(f"APIåŸå§‹å›æ‡‰å…§å®¹ï¼ˆé JSONï¼‰ï¼š{api_raw_response}")
                    except Exception as ee:
                        logger.error(f"å–å¾— resp.text() æ™‚åˆç™¼ç”ŸéŒ¯èª¤ï¼š{ee}")
                else:
                    logger.error(f"å°šæœªå–å¾— response (resp is None)ï¼Œå¯èƒ½æ˜¯é€£ç·šå¤±æ•—æˆ–ç¶²è·¯å•é¡Œã€‚Exception: {e}")
                logger.warning(f"Exception on fetch_comments attempt {attempt}/{self.retries} for offset {offset}: {e}")
            await asyncio.sleep(self.retry_delay)
        logger.error(f"Failed to fetch comments after {self.retries} attempts for offset {offset}")
        return {"error": "å¤šæ¬¡é‡è©¦å¤±æ•—ï¼Œå¯èƒ½æ˜¯ç¶²è·¯å•é¡Œæˆ–è¢«è¦çš®å°é–", "api_raw_response": api_raw_response}

    async def crawl_all_comments(self, shopid, itemid):
        api_raw_response = None
        async with aiohttp.ClientSession() as session:
            offset = 0
            limit = 50
            total_count = 1
            while offset < total_count:
                logger.info(f"Fetching comments offset {offset}")
                data = await self.fetch_comments(session, shopid, itemid, offset, limit)
                if isinstance(data, dict):
                    api_raw_response = data.get("api_raw_response", api_raw_response)
                    if "error" in data and data["error"]:
                        logger.error(f"âš ï¸ {data['error']}")
                        return self.comments, api_raw_response
                    data = data.get("data")
                if not data:
                    logger.error("âš ï¸ è¦çš®ä¼ºæœå™¨æ²’æœ‰å›æ‡‰ï¼Œå¯èƒ½æ˜¯è¢«æ“‹ä¸‹ï¼ˆHTTP éŒ¯èª¤æˆ–ç¶²è·¯å•é¡Œï¼‰")
                    return self.comments, api_raw_response

                total_count = data.get("data", {}).get("total", 0)
                ratings = data.get("data", {}).get("ratings", [])
                if not ratings:
                    logger.info("ğŸ” æ‰¾ä¸åˆ°ä»»ä½•ç•™è¨€ï¼Œå¯èƒ½æ˜¯å•†å“æ²’æœ‰ç•™è¨€æˆ–æœªå…¬é–‹")
                    return self.comments, api_raw_response

                for rating in ratings:
                    try:
                        comment_obj = CommentData.from_api(rating)
                        self.comments.append(comment_obj.dict())
                    except ValidationError as ve:
                        logger.error(f"Validation error: {ve}")

                offset += limit
            logger.info(f"Total comments fetched: {len(self.comments)}")
        return self.comments, api_raw_response
    
    def analyze_comments(self):
        if not self.comments:
            logger.warning("ç„¡ç•™è¨€è³‡æ–™å¯åˆ†æ")
            return None

        df = pd.DataFrame(self.comments)
        # æ˜Ÿç­‰çµ±è¨ˆ
        star_counts = df['rating_star'].value_counts().sort_index()
        logger.info("å„æ˜Ÿç­‰ç•™è¨€æ•¸é‡ï¼š")
        logger.info(star_counts.to_string())

        # ç•™è¨€æ™‚é–“è¶¨å‹¢åœ–
        df['comment_time_str'] = pd.to_datetime(df['comment_time_str'])
        df.set_index('comment_time_str', inplace=True)
        df_by_day = df.resample('D').size()

        fig, ax = plt.subplots(figsize=(12, 6))
        sns.lineplot(x=df_by_day.index, y=df_by_day.values, ax=ax)
        ax.set_title("ç•™è¨€æ™‚é–“è¶¨å‹¢ï¼ˆæ¯æ—¥ç•™è¨€æ•¸ï¼‰")
        ax.set_xlabel("æ—¥æœŸ")
        ax.set_ylabel("ç•™è¨€æ•¸")
        plt.tight_layout()

        return fig  # å›å‚³åœ–è¡¨

def run_async(coro):
    try:
        return asyncio.run(coro)
    except RuntimeError:
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(coro)

    

def parse_shopee_url(url):
    m1 = re.search(r"i\.(\d+)\.(\d+)", url)
    m2 = re.search(r"product/(\d+)/(\d+)", url)
    if m1:
        return m1.group(1).strip(), m1.group(2).strip()
    elif m2:
        return m2.group(1).strip(), m2.group(2).strip()
    else:
        return None, None

async def main(product_url: str, cookie: str = "", proxy: str = None):
    shopid, itemid = parse_shopee_url(product_url)
    if not shopid or not itemid:
        error_msg = "âŒ ç„¡æ³•è§£æå•†å“ç¶²å€ï¼Œè«‹ç¢ºèªæ ¼å¼ç‚º https://shopee.tw/i.åº—å®¶ID.å•†å“ID æˆ– https://shopee.tw/product/åº—å®¶ID/å•†å“ID"
        logger.error(error_msg)
        return None, None, error_msg, None

    crawler = ShopeeCommentCrawler(cookie=cookie, product_url=product_url, proxy=proxy)
    # ä¸‹é¢é€™è¡Œè¦è®“ crawl_all_comments å›å‚³ (comments, api_raw_response)
    comments, api_raw_response = await crawler.crawl_all_comments(shopid, itemid)

    if not comments or len(comments) == 0:
        return None, None, "âŒ æœªèƒ½æˆåŠŸå–å¾—ç•™è¨€ï¼Œå¯èƒ½æ˜¯å•†å“è¢«æ“‹æˆ–æ²’æœ‰å…¬é–‹ç•™è¨€", api_raw_response

    try:
        fig = crawler.analyze_comments()
        df = pd.DataFrame(crawler.comments)
        return fig, df, None, api_raw_response  # æ²’éŒ¯èª¤
    except Exception as e:
        logger.exception("åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤")
        return None, None, f"âŒ åˆ†æç•™è¨€æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}", api_raw_response

def run_review_report():
    st.header("ğŸ—£ï¸ è¦çš®å•†å“ç•™è¨€åˆ†æå™¨")
    show_cookie_instructions()

    product_url = st.text_input("è«‹è¼¸å…¥è¦çš®å•†å“ç¶²å€ï¼ˆä¾‹å¦‚ï¼šhttps://shopee.tw/product/12345678/87654321 æˆ– https://shopee.tw/i.12345678.87654321ï¼‰")
    cookie = st.text_area(
        "è«‹è¼¸å…¥ Cookie å­—ä¸²ï¼ˆå»ºè­°å¾ç€è¦½å™¨è¤‡è£½è²¼ä¸Šï¼Œç†±é–€å•†å“å»ºè­°è²¼ Cookieï¼Œå¦å‰‡é è¨­ç”¨å…§å»º Cookieï¼ŒæˆåŠŸç‡è¼ƒä½ï¼‰",
        height=100
    )
    proxy = st.text_input("ä»£ç†ä¼ºæœå™¨ Proxyï¼ˆæ ¼å¼ï¼šhttp://IP:PORTï¼Œéå¿…å¡«ï¼‰")
    
    cookie = cookie.replace('\n', '').replace('\r', '')




    if st.button("é–‹å§‹åˆ†æç•™è¨€"):
        if not product_url:
            st.warning("è«‹å…ˆè¼¸å…¥å•†å“ç¶²å€")
            return

        with st.spinner("åˆ†æä¸­ï¼Œè«‹ç¨å€™..."):
            try:
                fig, df, error, api_raw_response = run_async(main(product_url, cookie, proxy))
                if error:
                    st.error(f"ç•™è¨€åˆ†æå¤±æ•—ï¼š{error}")
                    st.expander("API åŸå§‹å›æ‡‰å…§å®¹ï¼ˆdebugç”¨ï¼‰").write(api_raw_response)
                    return
                if fig is None or df is None:
                    st.error("ç•™è¨€åˆ†æå¤±æ•—ï¼šæœªçŸ¥éŒ¯èª¤")
                    st.expander("API åŸå§‹å›æ‡‰å…§å®¹ï¼ˆdebugç”¨ï¼‰").write(api_raw_response)
                    return

                st.success("ç•™è¨€åˆ†æå®Œæˆï¼")
                if fig is not None:
                    st.pyplot(fig)
                    plt.close(fig)
                st.subheader("ç•™è¨€æ¨£æœ¬ï¼ˆå‰ 10 ç­†ï¼‰")
                if df is not None and not df.empty:
                    st.dataframe(df.head(10))
                    st.download_button("ä¸‹è¼‰ç•™è¨€ CSV", data=df.to_csv(index=False).encode("utf-8-sig"),
                                       file_name="shopee_comments.csv", mime="text/csv")
                else:
                    st.warning("æ²’æœ‰ç•™è¨€æ¨£æœ¬å¯é¡¯ç¤º")
                st.expander("API åŸå§‹å›æ‡‰å…§å®¹ï¼ˆdebugç”¨ï¼‰").write(api_raw_response)
            except Exception as e:
                st.error(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

if __name__ == "__main__":
    run_review_report()
    
    
    
    
